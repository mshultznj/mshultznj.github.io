---
title: "London_Final"
author: "Michael Shultz"
date: "May 11, 2018"
output: html_document
---
##Background and Context
Police are an integral part of the modern state. They help keep the peace in communities, and in many cases provide community services aimed at building trust and relationships with those they are tasked with serving. However, in many places large parts of these communities do not feel like they can trust the people that are supposed to be protecting them. In fact, police in the United Kingdom have [hovered at around a 60% trust rate for the past 31 years](http://www.bbc.com/news/magazine-26730705). This is a large chunk of the community that does not have faith in those that should be aimed at keeping their neighborhoods safe. I aim to look at stop and search data from London between the years of 2014 and 2017 to see if the police do a good job at predicting those they need to search, and whether the low trust in the force is justified or more people should trust their local bobbies. Through this process, I will detail my journey through the data science pipeline, and hopefully by the end you, the reader, will have learned a little bit about both data science and the city of London. 

##Getting our data
This data comes from the following kaggle page: [https://www.kaggle.com/sohier/london-police-records](https://www.kaggle.com/sohier/london-police-records). The first thing we need to do is read in our data. We save it as a CSV file, and load it into a dataframe.
```{r setup, message=FALSE, warning=FALSE}
library('dplyr')
library('readr')
library('tidyr')
library('ggplot2')

london_df <- read_csv("london-stop-and-search.csv")
london_df
```

This dataset looks pretty nice. Our data types seem to make sense, with a datetime for our date and string representations for everything else. However, when we look a little closer we can see some things that could make our data analysis a little easier. Editing our data in a way that makes it easier for us to process is known as data tidying, and is essential for good data analysis. Let's start by turning our true/false attributes into logicals.
```{r booleaning}
london_df$`Removal of more than just outer clothing` <- as.logical(london_df$`Removal of more than just outer clothing`)
london_df$`Outcome linked to object of search` <- as.logical(london_df$`Outcome linked to object of search`)
london_df$`Part of a policing operation` <- as.logical(london_df$`Part of a policing operation`)
```

Having logicals makes it easier to compare true/false attributes than having them in strings. The next thing we want to look at is if there is any data that is missing too much to be useful. Every value for the "Part of a policing operartion" attribute is either FALSE or NA. Though this could say something about the types of searches the police are carrying out, this attribute and its related, completely empty, attribute "Policing operation" are not providing us any useful information, and thus we can get rid of them.
```{r remove_attr}
london_df <- london_df %>%
  select(-`Part of a policing operation`, -`Policing operation`)
```

Looking at the "Self-defined ethnicity" attribute, we can see that each ethnicity comes with an ID comprised of a letter and a number. Let's break that ID into a separate attribute to make it easier to work with later.
```{r ethnicity_break}
#Extract data based off the fact that the ID is stored in parentheses
london_df <- london_df %>%
  extract(`Self-defined ethnicity`, into=c('self_defined_ethnicity', 'sde_id'), '(.*)[(](..)[)]')
```

Finally, we need to decide what to do with our missing data in other areas. Our main focus of this analysis will be ethnicity and age, so we'll focus on those areas. With 302,613 data points, it feels like we should be safe simply dropping this missing data, so let's try that.
```{r drop_data}
#Change Not Stated data to NA
london_df$sde_id[london_df$sde_id=="NS"]=NA
#Remove NA values for ethnicity and age
london_df <- london_df[complete.cases(london_df[ ,5:9]),]

london_df
```
Still 262,921 data points, more than enough to carry out our analysis. It looks like our data's in pretty good shape now, so let's carry on with our analysis.

##Exploring our data
Now that we have our data in a nice tidy format, we can do some exploratory data analysis. This is the process in which we start to analyze general trends and patterns in our data to get some testable hypothesis. We have a couple different data points that could provide us with interesting information, so lets plot a couple of them and see what we get. To plot, we'll be using R's ggplot2 library, which allows us to make all sorts of graphs.

Let's start by looking at what tends to happen in the aftermath of a search.
```{r outcome, message=FALSE, warning=FALSE}
london_df %>%
  ggplot(aes(x=Outcome, fill=Outcome)) +
  geom_bar() + 
  labs(title = "Outcome of searches") + 
  geom_text(stat='count', aes(label=..count..)) + 
  #We blank out the labels here since they all blended together at the bottom anyways.
  #The bars are still in the order they appear in the key
  theme(axis.text.x=element_blank())
```

Seems like a large majority of searches end up resulting in no action being taken, though when an action is taken they are usually arrested. Also seems like you're much more likely to get a warning for a drug related offense than any other offense.

Let's take a look at gender distributions next.
```{r gender, message=FALSE, warning=FALSE}
london_df %>%
  ggplot(aes(x=Gender, fill=Gender)) +
  geom_bar() + 
  labs(title = "Gender of suspects") + 
  geom_text(stat='count', aes(label=..count..)) 
```

About 93% of people stopped are male. This could be interesting to return to later.

Now a look at the age breakdown.
```{r age, message=FALSE, warning=FALSE}
london_df %>%
  ggplot(aes(x=`Age range`, fill=`Age range`)) +
  geom_bar() + 
  labs(title = "Age of suspects") + 
  geom_text(stat='count', aes(label=..count..)) 
```

No simple majority here, though there is a clear group that is most commonly stopped.

And finally let's look at ethnicities, both self and officer reported
```{r ethnicity, message=FALSE, warning=FALSE}
london_df %>%
  ggplot(aes(x=`Officer-defined ethnicity`, fill=`Officer-defined ethnicity`)) +
  geom_bar() + 
  labs(title = "Officer reported ethnicity of suspects") +
  geom_text(stat='count', aes(label=..count..))  

london_df %>% 
  ggplot(aes(x=sde_id, fill=sde_id)) +
  geom_bar() + 
  labs(title = "Self defined ethnicity IDs") +
  geom_text(stat='count', aes(label=..count..))  
```

These graph feels like it needs some context. A look at the [2011 London census diversity report](https://files.datapress.com/london/dataset/2011-census-diversity/2011-census-diversity-in-london.pdf) shows that White makes up about 4.5 million people in the population, while Black makes up only about 1 million people. Thus we have a disparity in the percentage of each ethnicity that is getting stopped. It is also interesting to look at the difference in self-reported ethnicities vs officer-reported ethnicities. It seems that the categories the officers are using are generalizations of a decent amount of self-percieved diversity in the community. One of the most striking numbers is the mixed category. In the officer's eyes, there was 1 mixed ethnicity person stopped. However, over 10,000 people self-reported as being mixed ethnicity (represented by IDs starting with an M).

##Ethnicity Correlation

While looking at graphs of our data can be interesting, it doesn't tell us much about relations between some of our attributes. Let's take a look at some relations and see if there's anything going on.

Let's start by seeing if the outcomes of the stops of different races have the same proportions as the overall rates. We'll create a new dataframe to hold our results.
```{r ethnic_stop, message=FALSE, warning=FALSE}
#Calculate overall proportions of search outcomes
none_prop <- length(which(london_df$Outcome == "Nothing found - no further action"))/length(london_df$Outcome)
local_prop <- length(which(london_df$Outcome == "Local resolution"))/length(london_df$Outcome)
unav_prop <- length(which(london_df$Outcome == "Article found - Detailed outcome unavailable"))/length(london_df$Outcome)
caution_prop <- length(which(london_df$Outcome == "Offender cautioned"))/length(london_df$Outcome)
warning_prop <- length(which(london_df$Outcome == "Offender given drugs possession warning"))/length(london_df$Outcome)
penalty_prop <- length(which(london_df$Outcome == "Offender given penalty notice"))/length(london_df$Outcome)
arrest_prop <- length(which(london_df$Outcome == "Suspect arrested"))/length(london_df$Outcome)
court_prop <- length(which(london_df$Outcome == "Suspect summonsed to court"))/length(london_df$Outcome)
total_v <- c("total", unav_prop, local_prop, none_prop, caution_prop, warning_prop, penalty_prop, arrest_prop, court_prop)

#Create our dataframe
ethnicity_outcomes <- data.frame(ethnicity="Totals", unavailable=unav_prop, local_res=local_prop, no_action=none_prop, cautioned=caution_prop, possession_warning=warning_prop, penalty_notice=penalty_prop, arrest=arrest_prop, court_summons=court_prop)

#Calculate individual ethnicity proportions
for (i in unique(london_df$`Officer-defined ethnicity`)){
  none_prop <- length(which(london_df$Outcome == "Nothing found - no further action" & london_df$`Officer-defined ethnicity` == i))/length(which(london_df$`Officer-defined ethnicity` == i))
  local_prop <- length(which(london_df$Outcome == "Local resolution" & london_df$`Officer-defined ethnicity` == i))/length(which(london_df$`Officer-defined ethnicity` == i))
  unav_prop <- length(which(london_df$Outcome == "Article found - Detailed outcome unavailable" & london_df$`Officer-defined ethnicity` == i))/length(which(london_df$`Officer-defined ethnicity` == i))
  caution_prop <- length(which(london_df$Outcome == "Offender cautioned" & london_df$`Officer-defined ethnicity` == i))/length(which(london_df$`Officer-defined ethnicity` == i))
  warning_prop <- length(which(london_df$Outcome == "Offender given drugs possession warning" & london_df$`Officer-defined ethnicity` == i))/length(which(london_df$`Officer-defined ethnicity` == i))
  penalty_prop <- length(which(london_df$Outcome == "Offender given penalty notice" & london_df$`Officer-defined ethnicity` == i))/length(which(london_df$`Officer-defined ethnicity` == i))
  arrest_prop <- length(which(london_df$Outcome == "Suspect arrested" & london_df$`Officer-defined ethnicity` == i))/length(which(london_df$`Officer-defined ethnicity` == i))
  court_prop <- length(which(london_df$Outcome == "Suspect summonsed to court" & london_df$`Officer-defined ethnicity` == i))/length(which(london_df$`Officer-defined ethnicity` == i))
  #Add it to our dataframe
  tmp <- c(i, unav_prop, local_prop, none_prop, caution_prop, warning_prop, penalty_prop, arrest_prop, court_prop)
  ethnicity_outcomes <- rbind(ethnicity_outcomes,tmp)
}
#Fix some data issues
ethnicity_outcomes$ethnicity <- c("Totals", unique(london_df$`Officer-defined ethnicity`))
ethnicity_outcomes
```

Some interesting data points to look at from this. Our no action values seem to be pretty similar, but our percentages for the solutions taken seem to vary a decent amount. Local resolution has a higher chance of occuring for those the officers label White, while Asians are more likely to recieve a penalty notice. Maybe we can predict what solution an officer will take based off of someone's ethnicity. Let's try!