---
title: "NYSE Analysis"
author: "Michael Shultz"
date: "May 17, 2018"
output: html_document
---

##Background and Context
Data science is an integral part of many fields now. Pretty much anywhere you look people have data they need analyzed. One field this that requires this even more than others is the stock market. People working on the stock market have a constant influx of data all day, and need to be able to utilize it to make the right decisions with their investments. Today, I'm going to walk you through the data science pipeline through the lens of the stock market, and maybe by the end we'll be able to find a nice investment for us both to make some money.

##Getting our data
This data comes from the following kaggle page: [https://www.kaggle.com/dgawlik/nyse/data](https://www.kaggle.com/dgawlik/nyse/data). The first thing we need to do is load our libraries and read in our data. The file is saved as a CSV ( [comma separated values](https://en.wikipedia.org/wiki/Comma-separated_values)) file, and load it into a dataframe.
```{r setup, message=FALSE, warning=FALSE}
library('dplyr')
library('readr')
library('tidyr')
library('ggplot2')
library('broom')
library('lubridate')
library('caret')
library('randomForest')
library('tibble')
library('ROCR')

se_df <- read_csv("prices.csv")
se_df
```

Here we have a dataframe with 7 attributes. Our 7 attributes are: 

Attribute    | Description                                   
------------ | -------------------------------------------------------
date         | Date of observed data
symbol       | New York Stock Exchange (NYSE) company symbol  
open         | Price per share at open              
close        | Price per share at close
low          | Lowest price per share on given day
high         | Highest price per share on given day
volume       | Total number of stocks for that company

The NYSE uses a set of symbols to represent their companies. These 1-4 letter representations make it faster for stockbrokers to communicate, and a list of their meanings can be found on the [NASDAQ website](https://www.nasdaq.com/screening/company-list.aspx).

##Tidying our data
Before we go into our analysis, we have a couple things we can change about our dataset to make it a little easier to work with and analyze. This process of tidying our data is an important part of the data science pipeline, as it makes sure we can easily handle whatever data we receive. 

First, we'll change our date attribute from a datetime to a date. All of our data has the time set to 00:00:00, so we are not losing anything by making this conversion. We're also going to create attributes for our year, month, and date to make them easier to access later.
```{r dt_to_date}
se_df$date <- as.Date(se_df$date)
se_df <- se_df %>%
  mutate(year=as.integer(format(se_df$date,"%Y"))) %>%
  mutate(month=as.integer(format(se_df$date,"%m"))) %>%
  mutate(day=as.integer(format(se_df$date,"%d")))
```

Second, we'll trim down our data a little bit. 850,000 data points is more than we need for our demonstration, so we'll cut some of the fat out. Our dataset currently holds data from 2010-2016, but for now we're just going to look in the short term and narrow it down to one year.
```{r range_narrow}
se_df <- se_df[!(se_df$year<2016),]
```

To make sure we have enough information for our analysis, we can look at the counts of each symbol to see that we still have enough data on all of our individual entries.
```{r counts}
count <- count(se_df, symbol) %>%
  #Sorts our data in increasing order
  arrange(n)
count
```
Our least tracked symbol still has 126 entries, so I think our analysis will be fine.


Finally, we'll create an attribute that describes whether our stock price has gone up or down since the previous date. Note that not everydate is represented in this dataset, so we'll have to take that into consideration when calculating the change. Also note that we use a positive integer to indicate an increase, and a negative number to indicate a decrease

```{r price_change, warning=FALSE}
#Sort our dataframe by symbol and date
se_df <- se_df %>% 
  arrange(symbol, date) %>%
  #Holder value for our variable
  mutate(percent_change = 0.0)
#Calculate change
for (i in 1:nrow(se_df)){
  if (i==1 || !(se_df$symbol[i]==se_df$symbol[i-1])){
    #Base case for first dates
    se_df$percent_change[i] <- 0.0
  }else{
    #We'll use closing values to track change
    #Percent change is difference in values divided by previous value times 100
    se_df$percent_change[i] <- ((se_df$close[i]-se_df$close[i-1])/se_df$close[i-1])*100
  }
}
```

And let's take one final look at our finished product.
```{r viewer}
se_df
```

And there we go! Our dataset now tracks stocks and their values over the course of 2016, and tells us how it changed over time.

##Exploring our data

Now that our data looks nice, it's time for us to actually do something with it. The first step in this process is exploratory data analysis, or EDA. EDA is the process of looking through our data to start to see if we can observe some patterns in it. We hope that by looking at our data, we can make better decisions for our statistical and machine learning methods.

Let's start by looking at price changes over time. First, let's look at our change in the average closing price of the market over time. We'll be using the ggplot2 library to create our plots.
```{r month_plot}
se_df %>%
  #Look at the data for each date together
  group_by(date) %>%
  #Calculate the average of the closing price
  mutate(average = mean(close)) %>%
  ggplot(aes(x=date, y=average)) +
  #Use a line graph to plot change over time
  geom_line() + 
  labs(title="Change in average market price over time",
       x="Date", y="Average market price")
```

Seems like we've had a decent amount of upward trend in the past year. The average stock price started at around 82.5, and has increased to a little under 90. 

Now we'll look at how individual months behaved. To do this, we'll use a boxplot. Boxplots give us information on 5 important statistical values: The minimum, the first quartile, the median, the third quartile, and the max. This allows us to see some more information on how individual months behaved for the stock market.
```{r month}
se_df %>%
  group_by(date) %>%
  mutate(average = mean(close)) %>%
  ggplot(aes(x=factor(month), y=average)) +
  geom_boxplot() + 
  labs(title="Change in average market price over time",
       x="Date", y="Average market price")
```

Seems like certain months have a lot of variability, while others are more constant. Variability in a month can be seen from the total length of the plot, as the farther apart the ends are the more spread the minimum and maximum average for the month were. What's interesting to note is we have 3 months with clear bottom outliers, dots separated from the rest of the graph, but no top outliers. So it seems like the market is unlikely to have an unusually good day, but there are chance of it having unusually bad days. 


Sometimes we want to look at how we're changing on a day to day basis. Let's look at how the change in percent_change of the overall market varies over time.
```{r trends}
se_df %>%
  group_by(date) %>%
  mutate(average = mean(percent_change)) %>%
  ggplot(aes(x=date, y=average)) +
  geom_line() +
  labs(title="Market change by date",
       x="Date", y="Market change")
```

Some interesting data in this graph. The market doesn't seem to stay growing or shrinking for a very consistent period of time. Instead it tends to fluctuate back and forth. There was a decent spike around late January-early February (perhaps related to the inauguration?) and a very deep dive right before July. While these might be interesting to explore in a different setting, for right now we'll just take note of them and continue on.

##Machine learning
Now that we've explored some of our data, it's time to see if we can put it to use! We want to see if we can use the data we have to predict what will happen in the future using machine learning. Machine learning is just a way for us to "train" our computer to recognize patterns in the data, and use these patterns to predict what will happen later.

The first thing we need to do is choose a hypothesis to predict. Now it should be clear that the stock market can already be at least somewhat predicted. If that wasn't possible, a lot of day traders would be out of luck! But what we can do is see if we can find a better way of predicting the market. What we'll do is test if making a prediction with raw data we have is better or worse than predicting with data standardized in relation to each company.

The first thing we want to do is turn our market change into a binomial variable. A binomial variable is simply a variable where there are only two possible options, in this case increase or decrease, that has been sampled multiple times (more info about binomial variables can be found [here](https://en.wikipedia.org/wiki/Binomial_distribution)). 
```{r to_bin}
se_df <- se_df %>% mutate(Direction=ifelse(percent_change>=0, "up", "down"))
```

Next, we need to create a standardized set of data for our analysis to use. We'll shorte our dataset a bit to improve calculation times.
```{r standardize}
std_df <- se_df %>%
  filter(month %in% 7:9) %>%
  group_by(symbol) %>%
  #Get mean of each company
  mutate(mean_change = mean(percent_change)) %>%
  #Standard deviation of each company
  mutate(sd_change = sd(percent_change)) %>%
  #Calculate the standardized score
  mutate(z_change = (percent_change-mean_change)/sd_change) %>%
  #Ungroup our data
  ungroup()
std_df
```

Looks good so far. Now for our analysis, we're going to use 15-fold cross validation on a random forest. A random forest is a collection of decision trees, trees that use the data they're provided to predict an outcome. A random forest simply builds a large amount of these trees, and averages their predictions to come up with its answer. Cross validation is a way for us to sample our data multiple times to ensure that our error rate remains low. More about cross validation [here](https://www.openml.org/a/estimation-procedures/1), and random forests [here](https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd). 

The first thing we do is create a wide dataset by using the tidyr::spread function.
```{r widen}
wide_se <- se_df %>%
  filter(month %in% 7:9) %>%
  dplyr::select(date, symbol, percent_change) %>%
  tidyr::spread(date, percent_change)
wide_std <- std_df %>%
  dplyr::select(date, symbol, z_change) %>%
  tidyr::spread(date, z_change)
```

Next, we calculate our differences.

```{r magic}
third_quarter <- se_df %>% filter(month %in% 7:9)
#Calculate it for our raw data
matrix_1 <- wide_se %>%
  dplyr::select(-symbol) %>%
  as.matrix() %>%
  .[,-1]

matrix_2 <- wide_se %>%
  dplyr::select(-symbol) %>%
  as.matrix() %>%
  .[,-ncol(.)]

diff_df <- (matrix_1 - matrix_2) %>%
  magrittr::set_colnames(NULL) %>%
  as_data_frame() %>%
  mutate(symbol=wide_se$symbol)

#Add the predicting outcome
df_norm <- diff_df %>%
  inner_join(third_quarter %>% dplyr::select(symbol, Direction), by="symbol") %>%
  mutate(Direction=factor(Direction, levels=c("down","up")))

#Calculate for our standardized data
matrix_1 <- wide_std %>%
  dplyr::select(-symbol) %>%
  as.matrix() %>%
  .[,-1]

matrix_2 <- wide_std %>%
  dplyr::select(-symbol) %>%
  as.matrix() %>%
  .[,-ncol(.)]

diff_df <- (matrix_1 - matrix_2) %>%
  magrittr::set_colnames(NULL) %>%
  as_data_frame() %>%
  mutate(symbol=wide_se$symbol)

#Add the predicting outcome
df_std <- diff_df %>%
  inner_join(third_quarter %>% dplyr::select(symbol, Direction), by="symbol") %>%
  mutate(Direction=factor(Direction, levels=c("down","up")))

df_std
df_norm
```

Now that everything is set up, it's time to construct our forests. Since we have a lot of data, we'll do 15-folds to give us a better chance of high accuracy. We'll use 100 trees for each forest.
```{r treeeeees}
#Sets the seed for randomness
set.seed(1234)

#Test on our raw data first
result_df_norm <- createFolds(df_norm$Direction, k=15) %>%
  #Apply a function to our dataset
  purrr::imap(function(test_indices, fold_number){
    #Set part of our data as training data
    train_df <- df_norm %>%
      dplyr::select(-symbol) %>%
      slice(-test_indices)
    #Set the rest as our testing data
    test_df <- df_norm %>%
      dplyr::select(-symbol) %>%
      slice(test_indices)
    #Fit our random forest on our training data
    rf <- randomForest(Direction~., data=train_df, ntree=100, na.action = na.exclude)
    #Use our test data to add predicted values
    test_df %>%
      dplyr::select(observed_label = Direction) %>%
      mutate(fold=fold_number) %>%
      mutate(prob_positive_norm = predict(rf, newdata=test_df, type="prob")[, "up"]) %>%
      mutate(predicted_label_norm = ifelse(prob_positive_norm > 0.5, "up", "down"))
  }) %>%
  purrr::reduce(bind_rows) %>%
  rowid_to_column("ID")

#Next test on our standarized data
result_df_std <- createFolds(df_std$Direction, k=15) %>%
  #Apply a function to our dataset
  purrr::imap(function(test_indices, fold_number){
    #Set part of our data as training data
    train_df <- df_std %>%
      dplyr::select(-symbol) %>%
      slice(-test_indices)
    #Set the rest as our testing data
    test_df <- df_std %>%
      dplyr::select(-symbol) %>%
      slice(test_indices)
    #Fit our random forest on our training data
    rf <- randomForest(Direction~., data=train_df, ntree=100, na.action = na.exclude)
    #Use our test data to add predicted values
    test_df %>%
      dplyr::select(observed_label = Direction) %>%
      mutate(fold=fold_number) %>%
      mutate(prob_positive_std = predict(rf, newdata=test_df, type="prob")[, "up"]) %>%
      mutate(predicted_label_std = ifelse(prob_positive_std > 0.5, "up", "down"))
  }) %>%
  purrr::reduce(bind_rows) %>%
  rowid_to_column("ID")

#Merge our two experiments, and clean them up
names(result_df_norm)[2] <- "observed_norm"
names(result_df_std)[2] <- "observed_std"
result_df <- merge(result_df_norm, result_df_std, by="ID")
names(result_df)[3] <- "fold"
result_df <- result_df %>%
  dplyr::select(-fold.y)

result_df %>% head()
```
```{r na_omit, echo=FALSE}
result_df <- na.omit(result_df)
```
Now that we have our results, we can compute our error on each fold for each model.
```{r error_comp}
result_df %>%
  mutate(error_norm = (observed_norm != predicted_label_norm),
         error_std = (observed_std != predicted_label_std)) %>%
  group_by(fold) %>%
  summarize(norm_rf=mean(error_norm), std_rf=mean(error_std)) %>%
  tidyr::gather(model, error, -fold) %>%
  lm(error~model, data=.) %>%
  broom::tidy()
```

Finally, we'll create a Reciever Operating Characteristic (ROC) curve to see how our two plans lined up. An ROC curve tells us how good a binary classifier, in this case our predictor, is at doing its job. By calculating the area underneath the ROC curve (AUROC), we can tell how good of a job our predictor is doing. If you're interested in reading more in depth about them, you can read up on it [here](https://en.wikipedia.org/wiki/Receiver_operating_characteristic).
```{r roc}
#Create a list of labels for true observed
labels_std <- split(result_df$observed_std, result_df$fold)
labels_norm <- split(result_df$observed_norm, result_df$fold)

#Create a list of predictions for our two random forests
predictions_std <- split(result_df$prob_positive_std, result_df$fold) %>% prediction(labels_std)
predictions_norm <- split(result_df$prob_positive_norm, result_df$fold) %>% prediction(labels_norm)

#Compute average AUROCs
mean_auc_std <- predictions_std %>%
  performance(measure="auc") %>%
  slot("y.values") %>% unlist() %>%
  mean()
mean_auc_norm <- predictions_norm %>%
  performance(measure="auc") %>%
  slot("y.values") %>% unlist() %>%
  mean()

#Plot our ROC curves
predictions_std %>%
  performance(measure="tpr", x.measure="fpr") %>%
  plot(avg="threshold", col="orange", lwd=2)
predictions_norm %>%
  performance(measure="tpr", x.measure="fpr") %>%
  plot(avg="threshold", col="blue", lwd=2, add=TRUE)
#Add a legend
legend("bottomright",
       legend=paste(c("std", "norm"), "data, AUC:",
                    round(c(mean_auc_std,mean_auc_norm),digits=3)),
       col=c("orange","blue"))
```

Thus our random forests are right about 49% of the time with our standardized data, and 48.6% of the time for the normal data. Obviously, these percentages are not the type of values we would like to see if we were going to use this to predict whether the stock market would increase or decrease on a given day. We'd be better off flipping a coin! There are multiple reasons this could happen. Part of it could be the fact that in order to lower the amount of data we only looked at a three month timespan. Perhaps lowering the amount of stocks we looked at would have been a better method of data trimming. It's also possible that using a linear method like this is simply not good for predicting the stock market. It is a complex system that relies on public opinion and current news to change, and it's possible that simply looking at past data is not a good enough indicator.

##Hypothesis testing
Regardless of the lack of accuracy of our forests, we still should look at if our hypothesis was good. Our question we wanted to answer was if standardizing the percent change in price for each stock would affect our prediction accuracy. If this was true, we would reject the null hypothesis that the two methods were equal, otherwise we would accept the null hypothesis. For our purposes, we will say that if p<0.05, we will reject the null hypothesis (more info on p values [here](http://www.dummies.com/education/math/statistics/what-a-p-value-tells-you-about-statistical-data/)). As we saw above, the p value for our test was 0.8568, significantly greater than 0.05. Thus, we would not reject the null hypothesis, and we conclude that standardizing percent changes does not help you better predict the stock market.

##Conclusion
The stock market is a wild place. People can make fortunes on playing the stock market correctly, but many people are just as likely to lose it all. While the analysis we did would not help us find success on the market, there's a large chunk of data here that we simply could not process. Perhaps using more data would allow us to better predict it. But maybe its just the data telling us that to not quit our day jobs, and the stock market isn't just a code for us to crack.